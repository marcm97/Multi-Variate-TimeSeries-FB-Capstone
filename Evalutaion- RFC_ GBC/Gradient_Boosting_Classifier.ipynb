{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Validation using Grandinet Boosting Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the ArticularyWordRecognition Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_series/_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_hierarchical/_check.py:50: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_hierarchical/_check.py:51: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_panel/_check.py:48: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_panel/_check.py:49: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n"
     ]
    }
   ],
   "source": [
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sktime.datatypes._panel._convert import from_nested_to_3d_numpy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from pyts.datasets import load_basic_motions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train= load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"train\", return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test= load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"test\", return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 9, 144)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3d = from_nested_to_3d_numpy(X_train)\n",
    "X_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 9, 144)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_3d= from_nested_to_3d_numpy(X_test)\n",
    "X_test_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reservoir_features:\n",
    "    '''\n",
    "    creates an object associated with a multivariate dataset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self,data,num_features):\n",
    "        '''\n",
    "        data: Takes in a multidimensional array (x * y * z) - z>y\n",
    "        Initializes it\n",
    "        x: Timeseries\n",
    "        y: Attributes for a given timeseries observation\n",
    "        z: timestamped observations (features)\n",
    "        \n",
    "        num_features: you must specify the dimension you want to reduce it to\n",
    "        \n",
    "        '''\n",
    "        self.features = []\n",
    "        self.filters_used = []\n",
    "        self.original_data = data.copy()\n",
    "        self.data = data.copy()\n",
    "        self.num_features = num_features\n",
    "        self.x = data.shape[0]\n",
    "        self.y = data.shape[1]\n",
    "        self.z = data.shape[2]\n",
    "        # perform checks \n",
    "        #1. 3d numpy array\n",
    "        #2. Each time series should have same number of observations\n",
    "        #3. num_features should be less than timestamped observations\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Each attribute could potentially be on a different scale\n",
    "        modifies the original data and performs a min max normalization\n",
    "        '''\n",
    "        for i in range(self.original_data.shape[0]):\n",
    "            for j in range(self.original_data.shape[1]):\n",
    "                self.data[i][j] = (self.original_data[i][j] - self.original_data[i][j].min())/(self.original_data[i][j].max()-self.original_data[i][j].min())\n",
    "    \n",
    "    \n",
    "    def filters(self,stride_len = [1], num_filters = 1):\n",
    "        '''\n",
    "        stride_len: num of columns to skip after each filter multiplication\n",
    "        num_filters: you can specify the number of filters you need; each filter will be of a differnt size\n",
    "        size of filter = n*m \n",
    "        (n = # of rows = attribute size, \n",
    "        m = # of columns)\n",
    "        '''\n",
    "        #Have error check to make sure stride len is a list and value is <length of attributes\n",
    "        n = self.y\n",
    "        \n",
    "        \n",
    "        #Edge case vals is empty/smaller than num_filters\n",
    "        \n",
    "        for iteration in range(num_filters):\n",
    "            m = self._get_m(stride_len[iteration])\n",
    "            filter_a = np.random.random((n,m))\n",
    "            print(\"filter of size \", str(n), \"*\", str(m), \"was created\\n\")\n",
    "            self.filters_used.append(filter_a)\n",
    "            \n",
    "            temp_features =[]\n",
    "            for i in range(self.x):\n",
    "                temp = []\n",
    "                j = 0\n",
    "                while j + m < self.data.shape[2]:\n",
    "                    temp.append((filter_a*self.data[i,:,j:j+m]).mean())\n",
    "                    j+=stride_len[iteration]\n",
    "                temp_features.append(temp)\n",
    "            self.features.append(temp_features)\n",
    "    \n",
    "    \n",
    "    def _get_m(self,stride_len):\n",
    "        '''\n",
    "        stride_len: \n",
    "        based on stride length,& num_features, we calculate possible filter size \n",
    "        '''\n",
    "        m = self.z -(self.num_features)*stride_len\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    def result_features(self):\n",
    "        '''\n",
    "        if multiple filters were added, takes the average result\n",
    "        '''\n",
    "        ans =[]\n",
    "        for timeseries in range(len(self.features[0])):\n",
    "            temp =[]\n",
    "            for feature in range(len(self.features[0][0])):\n",
    "                val = np.mean([self.features[filter][timeseries][feature] for filter in range(len(self.features))])\n",
    "                temp.append(val)\n",
    "            ans.append(temp)\n",
    "        return ans\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, num_features, stride_len, num_filters):\n",
    "    data_transformed = reservoir_features(data ,num_features = num_features)\n",
    "    #normalize\n",
    "    data_transformed.normalize()\n",
    "    #create 2 filters\n",
    "    data_transformed.filters(stride_len = stride_len, num_filters = num_filters)\n",
    "    data_transformed = data_transformed.result_features()\n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  9 * 24 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed= transform_data(X_3d, 40, [3], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  9 * 24 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed=transform_data(X_test_3d, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07272727272727272"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed, y_train)\n",
    "clf.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 1., ..., 1., 1., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 1., 2.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 2., 1., ..., 1., 1., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 1., 2.],\n",
       "        ...,\n",
       "        [1., 2., 1., ..., 1., 1., 1.],\n",
       "        [1., 2., 1., ..., 1., 1., 1.],\n",
       "        [1., 2., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 2., 1., ..., 1., 1., 1.],\n",
       "        [1., 2., 2., ..., 2., 1., 1.],\n",
       "        [1., 2., 2., ..., 2., 1., 2.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 2.],\n",
       "        ...,\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.]],\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 2., 1.],\n",
       "        [2., 1., 2., ..., 2., 2., 2.],\n",
       "        [2., 2., 2., ..., 2., 2., 1.],\n",
       "        ...,\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.]],\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 1.],\n",
       "        [2., 2., 2., ..., 2., 2., 2.],\n",
       "        ...,\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.],\n",
       "        [1., 2., 1., ..., 2., 2., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.apply(X_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinsind/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:1268: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(proba)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf,   0., -inf, ..., -inf, -inf, -inf],\n",
       "       [  0., -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       ...,\n",
       "       [-inf,   0., -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf,   0., -inf, ..., -inf, -inf, -inf],\n",
       "       [-inf,   0., -inf, ..., -inf, -inf, -inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_log_proba(X_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07272727272727272\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred=clf.predict(X_train_transformed)\n",
    "clf_acc_score = accuracy_score(y_train, clf_train_pred)\n",
    "print(clf_acc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5170454545454546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train, clf.predict_proba(X_train_transformed), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04489134274801395\n",
      "0.04489134274801394\n",
      "0.07272727272727272\n"
     ]
    }
   ],
   "source": [
    "train_f1 = f1_score(y_train, clf_train_pred, average='weighted')\n",
    "train_f2 = f1_score(y_train, clf_train_pred, average='macro')\n",
    "train_f3 = f1_score(y_train, clf_train_pred, average='micro')\n",
    "print(train_f1)\n",
    "print(train_f2)\n",
    "print(train_f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_test_pred=clf.predict(X_test_transformed)\n",
    "clf_test_acc_score = accuracy_score(y_test, clf_test_pred)\n",
    "print(clf_test_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5052083333333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf.predict_proba(X_test_transformed), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018380287708303657\n",
      "0.01838028770830366\n",
      "0.05000000000000001\n"
     ]
    }
   ],
   "source": [
    "#f1-score:\n",
    "test_f1 = f1_score(y_test, clf_test_pred, average='weighted')\n",
    "test_f2 = f1_score(y_test, clf_test_pred, average='macro')\n",
    "test_f3 = f1_score(y_test, clf_test_pred, average='micro')\n",
    "print(test_f1)\n",
    "print(test_f2)\n",
    "print(test_f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the AtrialFibrillation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2, 640)\n",
      "(15, 2, 640)\n"
     ]
    }
   ],
   "source": [
    "X_AF,y_AF= load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\", return_X_y=True)\n",
    "X_AF_test,y_AF_test= load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\", return_X_y=True)\n",
    "X_AF_3d = from_nested_to_3d_numpy(X_AF)\n",
    "X_AF_3d.shape\n",
    "X_AF_test_3d= from_nested_to_3d_numpy(X_AF_test)\n",
    "X_AF_test_3d.shape\n",
    "\n",
    "print(X_AF_3d.shape)\n",
    "print(X_AF_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  2 * 520 was created\n",
      "\n",
      "filter of size  2 * 520 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_AF = transform_data(X_AF_3d, 40, [3], 1)\n",
    "X_test_transformed_AF = transform_data(X_AF_test_3d, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_transformed_AF))\n",
    "print(len(X_train_transformed_AF[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_AF = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_AF, y_AF)\n",
    "clf_AF.score(X_train_transformed_AF, y_AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 2., 1.],\n",
       "        [2., 2., 2.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 2., 1.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 1., 2.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [2., 1., 2.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [2., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [2., 1., 2.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [2., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2., 2., 1.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 1., 2.],\n",
       "        ...,\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 1., 2.]],\n",
       "\n",
       "       [[2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        ...,\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.],\n",
       "        [2., 2., 2.]],\n",
       "\n",
       "       [[2., 1., 1.],\n",
       "        [2., 1., 2.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [2., 1., 1.],\n",
       "        [2., 1., 2.]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_AF.apply(X_train_transformed_AF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for Train Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 1.0\n",
      "f1_score:Weighted: 0.04489134274801395\n",
      "f1_score:macro: 0.04489134274801394\n",
      "f1_score:micro: 0.07272727272727272\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_AF=clf_AF.predict(X_train_transformed_AF)\n",
    "clf_acc_score_AF = accuracy_score(y_AF, clf_train_pred_AF)\n",
    "print(\"Accuracy_score:\",clf_acc_score_AF)\n",
    "\n",
    "train_AF_f1 = f1_score(y_AF, clf_train_pred_AF, average='weighted')\n",
    "train_AF_f2 = f1_score(y_AF, clf_train_pred_AF, average='macro')\n",
    "train_AF_f3 = f1_score(y_AF, clf_train_pred_AF, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_f1)\n",
    "print(\"f1_score:macro:\",train_f2)\n",
    "print(\"f1_score:micro:\",train_f3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_AF, clf_AF.predict_proba(X_train_transformed_AF), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy for Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.26666666666666666\n",
      "f1_score:Weighted: 0.2261904761904762\n",
      "f1_score:macro: 0.2261904761904762\n",
      "f1_score:micro: 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_AF=clf_AF.predict(X_test_transformed_AF)\n",
    "clf_acc_score_test_AF = accuracy_score(y_AF_test, clf_test_pred_AF)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_AF)\n",
    "\n",
    "# F1-Score:\n",
    "test_AF_f1 = f1_score(y_AF_test, clf_test_pred_AF, average='weighted')\n",
    "test_AF_f2 = f1_score(y_AF_test, clf_test_pred_AF, average='macro')\n",
    "test_AF_f3 = f1_score(y_AF_test, clf_test_pred_AF, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_AF_f1)\n",
    "print(\"f1_score:macro:\",test_AF_f2)\n",
    "print(\"f1_score:micro:\",test_AF_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_AF_test, clf_AF.predict_proba(X_test_transformed_AF), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Cricket dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 6, 1197)\n",
      "(72, 6, 1197)\n"
     ]
    }
   ],
   "source": [
    "X_C,y_C= load_UCR_UEA_dataset('Cricket', split=\"train\", return_X_y=True)\n",
    "X_C_test,y_C_test= load_UCR_UEA_dataset('Cricket', split=\"test\", return_X_y=True)\n",
    "X_C_3d = from_nested_to_3d_numpy(X_C)\n",
    "X_C_3d.shape\n",
    "X_C_test_3d= from_nested_to_3d_numpy(X_C_test)\n",
    "X_C_test_3d.shape\n",
    "\n",
    "print(X_C_3d.shape)\n",
    "print(X_C_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 597 was created\n",
      "\n",
      "filter of size  6 * 597 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_C = transform_data(X_C_3d, 200, [3], 1)\n",
    "X_test_transformed_C = transform_data(X_C_test_3d, 200, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518518518518519"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_C = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_C, y_C)\n",
    "clf_C.score(X_train_transformed_C, y_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 2., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 1., ..., 2., 1., 1.]],\n",
       "\n",
       "       [[2., 1., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 2., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.]],\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 2., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 2., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.]],\n",
       "\n",
       "       [[2., 1., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 2., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.]],\n",
       "\n",
       "       [[2., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 2., 1., ..., 2., 1., 1.],\n",
       "        [1., 1., 1., ..., 2., 1., 1.],\n",
       "        ...,\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 2., 1., ..., 2., 1., 1.],\n",
       "        [2., 1., 1., ..., 2., 1., 1.]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_C.apply(X_train_transformed_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8518518518518519\n",
      "f1_score:Weighted: 0.8473957844700878\n",
      "f1_score:macro: 0.8473957844700878\n",
      "f1_score:micro: 0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_C=clf_C.predict(X_train_transformed_C)\n",
    "clf_acc_score_C = accuracy_score(y_C, clf_train_pred_C)\n",
    "print(\"Accuracy_score:\",clf_acc_score_C)\n",
    "\n",
    "train_C_f1 = f1_score(y_C, clf_train_pred_C, average='weighted')\n",
    "train_C_f2 = f1_score(y_C, clf_train_pred_C, average='macro')\n",
    "train_C_f3 = f1_score(y_C, clf_train_pred_C, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_C_f1)\n",
    "print(\"f1_score:macro:\",train_C_f2)\n",
    "print(\"f1_score:micro:\",train_C_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9180695847362511"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_C, clf_C.predict_proba(X_train_transformed_C), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.5138888888888888\n",
      "f1_score:Weighted: 0.4891534391534391\n",
      "f1_score:macro: 0.48915343915343906\n",
      "f1_score:micro: 0.5138888888888888\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_C=clf_C.predict(X_test_transformed_C)\n",
    "clf_acc_score_test_C = accuracy_score(y_C_test, clf_test_pred_C)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_C)\n",
    "\n",
    "# F1-Score:\n",
    "test_C_f1 = f1_score(y_C_test, clf_test_pred_C, average='weighted')\n",
    "test_C_f2 = f1_score(y_C_test, clf_test_pred_C, average='macro')\n",
    "test_C_f3 = f1_score(y_C_test, clf_test_pred_C, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_C_f1)\n",
    "print(\"f1_score:macro:\",test_C_f2)\n",
    "print(\"f1_score:micro:\",test_C_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7544191919191919"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_C_test, clf_C.predict_proba(X_test_transformed_C), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Epilepsy Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3, 206)\n",
      "(138, 3, 206)\n"
     ]
    }
   ],
   "source": [
    "X_E,y_E= load_UCR_UEA_dataset('Epilepsy', split=\"train\", return_X_y=True)\n",
    "X_E_test,y_E_test= load_UCR_UEA_dataset('Epilepsy', split=\"test\", return_X_y=True)\n",
    "X_E_3d = from_nested_to_3d_numpy(X_E)\n",
    "X_E_3d.shape\n",
    "X_E_test_3d= from_nested_to_3d_numpy(X_E_test)\n",
    "X_E_test_3d.shape\n",
    "\n",
    "print(X_E_3d.shape)\n",
    "print(X_E_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  3 * 158 was created\n",
      "\n",
      "filter of size  3 * 158 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_E = transform_data(X_E_3d, 16, [3], 1)\n",
    "X_test_transformed_E = transform_data(X_E_test_3d, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_E = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_E, y_E)\n",
    "clf_E.score(X_train_transformed_E, y_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [1., 2., 2., 2.],\n",
       "        ...,\n",
       "        [2., 1., 2., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 2., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1.],\n",
       "        [2., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 2., 1., 1.],\n",
       "        [1., 2., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1.],\n",
       "        [2., 1., 1., 2.],\n",
       "        [2., 1., 1., 2.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 2., 1., 2.],\n",
       "        [1., 2., 2., 2.],\n",
       "        [1., 2., 2., 1.],\n",
       "        ...,\n",
       "        [2., 1., 1., 2.],\n",
       "        [2., 1., 1., 2.],\n",
       "        [2., 1., 2., 2.]],\n",
       "\n",
       "       [[1., 2., 1., 2.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [1., 2., 2., 1.],\n",
       "        ...,\n",
       "        [2., 1., 1., 2.],\n",
       "        [2., 1., 1., 2.],\n",
       "        [2., 1., 2., 2.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_E.apply(X_train_transformed_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 1.0\n",
      "f1_score:Weighted: 1.0\n",
      "f1_score:macro: 1.0\n",
      "f1_score:micro: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_E=clf_E.predict(X_train_transformed_E)\n",
    "clf_acc_score_E = accuracy_score(y_E, clf_train_pred_E)\n",
    "print(\"Accuracy_score:\",clf_acc_score_E)\n",
    "\n",
    "train_E_f1 = f1_score(y_E, clf_train_pred_E, average='weighted')\n",
    "train_E_f2 = f1_score(y_E, clf_train_pred_E, average='macro')\n",
    "train_E_f3 = f1_score(y_E, clf_train_pred_E, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_E_f1)\n",
    "print(\"f1_score:macro:\",train_E_f2)\n",
    "print(\"f1_score:micro:\",train_E_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_E, clf_E.predict_proba(X_train_transformed_E), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Test Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.2898550724637681\n",
      "f1_score:Weighted: 0.2827929970559522\n",
      "f1_score:macro: 0.2847686707030478\n",
      "f1_score:micro: 0.2898550724637681\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_E=clf_E.predict(X_test_transformed_E)\n",
    "clf_acc_score_test_E = accuracy_score(y_E_test, clf_test_pred_E)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_E)\n",
    "\n",
    "# F1-Score:\n",
    "test_E_f1 = f1_score(y_E_test, clf_test_pred_E, average='weighted')\n",
    "test_E_f2 = f1_score(y_E_test, clf_test_pred_E, average='macro')\n",
    "test_E_f3 = f1_score(y_E_test, clf_test_pred_E, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_E_f1)\n",
    "print(\"f1_score:macro:\",test_E_f2)\n",
    "print(\"f1_score:micro:\",test_E_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5117874602898749"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_E_test, clf_E.predict_proba(X_test_transformed_E), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the FingerMovements Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 28, 50)\n",
      "(208, 28, 50)\n"
     ]
    }
   ],
   "source": [
    "X_F,y_F= load_UCR_UEA_dataset('FingerMovements',return_X_y=True)\n",
    "X_F_Train,X_F_test,y_F_Train,y_F_test= train_test_split(X_F,y_F, test_size=0.5, random_state=0)\n",
    "X_F_3d = from_nested_to_3d_numpy(X_F_Train)\n",
    "X_F_3d.shape\n",
    "X_F_test_3d= from_nested_to_3d_numpy(X_F_test)\n",
    "X_F_test_3d.shape\n",
    "\n",
    "print(X_F_3d.shape)\n",
    "print(X_F_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  28 * 2 was created\n",
      "\n",
      "filter of size  28 * 2 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_F = transform_data(X_F_3d, 16, [3], 1)\n",
    "X_test_transformed_F = transform_data(X_F_test_3d, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9711538461538461"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_F = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_F, y_F_Train)\n",
    "clf_F.score(X_train_transformed_F, y_F_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.]],\n",
       "\n",
       "       [[2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.]],\n",
       "\n",
       "       [[2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.]],\n",
       "\n",
       "       [[2.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_F.apply(X_train_transformed_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Train Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.9711538461538461\n",
      "f1_score:Weighted: 0.9711485102023393\n",
      "f1_score:macro: 0.9711431742508324\n",
      "f1_score:micro: 0.9711538461538461\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_F=clf_F.predict(X_train_transformed_F)\n",
    "clf_acc_score_F = accuracy_score(y_F_Train, clf_train_pred_F)\n",
    "print(\"Accuracy_score:\",clf_acc_score_F)\n",
    "\n",
    "train_F_f1 = f1_score(y_F_Train, clf_train_pred_F, average='weighted')\n",
    "train_F_f2 = f1_score(y_F_Train, clf_train_pred_F, average='macro')\n",
    "train_F_f3 = f1_score(y_F_Train, clf_train_pred_F, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_F_f1)\n",
    "print(\"f1_score:macro:\",train_F_f2)\n",
    "print(\"f1_score:micro:\",train_F_f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Test Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.44711538461538464\n",
      "f1_score:Weighted: 0.4471281642334274\n",
      "f1_score:macro: 0.44710260499734183\n",
      "f1_score:micro: 0.44711538461538464\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_F=clf_F.predict(X_test_transformed_F)\n",
    "clf_acc_score_test_F = accuracy_score(y_F_test, clf_test_pred_F)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_F)\n",
    "\n",
    "# F1-Score:\n",
    "test_F_f1 = f1_score(y_F_test, clf_test_pred_F, average='weighted')\n",
    "test_F_f2 = f1_score(y_F_test, clf_test_pred_F, average='macro')\n",
    "test_F_f3 = f1_score(y_F_test, clf_test_pred_F, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_F_f1)\n",
    "print(\"f1_score:macro:\",test_F_f2)\n",
    "print(\"f1_score:micro:\",test_F_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43444290337494224"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_F_test, clf_F.predict_proba(X_test_transformed_F)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Handwriting Data Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3, 152)\n",
      "(500, 3, 152)\n"
     ]
    }
   ],
   "source": [
    "X_H,y_H= load_UCR_UEA_dataset('Handwriting', return_X_y=True)\n",
    "X_H_Train,X_H_test,y_H_Train,y_H_test= train_test_split(X_H,y_H, test_size=0.5, random_state=0)\n",
    "X_H_3d = from_nested_to_3d_numpy(X_H_Train)\n",
    "X_H_3d.shape\n",
    "X_H_test_3d= from_nested_to_3d_numpy(X_H_test)\n",
    "X_H_test_3d.shape\n",
    "\n",
    "print(X_H_3d.shape)\n",
    "print(X_H_test_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  3 * 122 was created\n",
      "\n",
      "filter of size  3 * 122 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_H = transform_data(X_H_3d, 10, [3], 1)\n",
    "X_test_transformed_H = transform_data(X_H_test_3d, 10, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_H = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_H, y_H_Train)\n",
    "clf_H.score(X_train_transformed_H, y_H_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 1., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 1., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.]],\n",
       "\n",
       "       [[1., 1., 2., ..., 2., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 2., ..., 2., 2., 2.],\n",
       "        [1., 1., 2., ..., 2., 2., 2.],\n",
       "        [1., 1., 2., ..., 2., 2., 2.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 1., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 1., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 2., 1.],\n",
       "        [2., 1., 2., ..., 1., 2., 1.],\n",
       "        [2., 1., 1., ..., 1., 2., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.],\n",
       "        [1., 1., 1., ..., 1., 2., 1.]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_H.apply(X_train_transformed_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.036\n",
      "f1_score:Weighted: 0.01800859242905443\n",
      "f1_score:macro: 0.017411040357140122\n",
      "f1_score:micro: 0.036\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_H=clf_H.predict(X_train_transformed_H)\n",
    "clf_acc_score_H = accuracy_score(y_H_Train, clf_train_pred_H)\n",
    "print(\"Accuracy_score:\",clf_acc_score_H)\n",
    "\n",
    "train_H_f1 = f1_score(y_H_Train, clf_train_pred_H, average='weighted')\n",
    "train_H_f2 = f1_score(y_H_Train, clf_train_pred_H, average='macro')\n",
    "train_H_f3 = f1_score(y_H_Train, clf_train_pred_H, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_H_f1)\n",
    "print(\"f1_score:macro:\",train_H_f2)\n",
    "print(\"f1_score:micro:\",train_H_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49737676775918715"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_H_Train, clf_H.predict_proba(X_train_transformed_H), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.038\n",
      "f1_score:Weighted: 0.005689746222678598\n",
      "f1_score:macro: 0.0055136480066801325\n",
      "f1_score:micro: 0.038\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_H=clf_H.predict(X_test_transformed_H)\n",
    "clf_acc_score_test_H = accuracy_score(y_H_test, clf_test_pred_H)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_H)\n",
    "\n",
    "# F1-Score:\n",
    "test_H_f1 = f1_score(y_H_test, clf_test_pred_H, average='weighted')\n",
    "test_H_f2 = f1_score(y_H_test, clf_test_pred_H, average='macro')\n",
    "test_H_f3 = f1_score(y_H_test, clf_test_pred_H, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_H_f1)\n",
    "print(\"f1_score:macro:\",test_H_f2)\n",
    "print(\"f1_score:micro:\",test_H_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4978271207564659"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_H_test, clf_H.predict_proba(X_test_transformed_H), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Basic Motions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 6, 100)\n",
      "(40, 6, 100)\n"
     ]
    }
   ],
   "source": [
    "X_B, X_B_test, y_B, y_B_test = load_basic_motions(return_X_y=True)\n",
    "\n",
    "\n",
    "print(X_B.shape)\n",
    "print(X_B_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 52 was created\n",
      "\n",
      "filter of size  6 * 52 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed_B = transform_data(X_B, 16, [3], 1)\n",
    "X_test_transformed_B = transform_data(X_B_test, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_B = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "                                  max_depth=1, random_state=0).fit(X_train_transformed_B, y_B)\n",
    "clf_B.score(X_train_transformed_B, y_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Train Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 1.0\n",
      "f1_score:Weighted: 1.0\n",
      "f1_score:macro: 1.0\n",
      "f1_score:micro: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_train_pred_B=clf_B.predict(X_train_transformed_B)\n",
    "clf_acc_score_B = accuracy_score(y_B, clf_train_pred_B)\n",
    "print(\"Accuracy_score:\",clf_acc_score_B)\n",
    "\n",
    "train_B_f1 = f1_score(y_B, clf_train_pred_B, average='weighted')\n",
    "train_B_f2 = f1_score(y_B, clf_train_pred_B, average='macro')\n",
    "train_B_f3 = f1_score(y_B, clf_train_pred_B, average='micro')\n",
    "print(\"f1_score:Weighted:\",train_B_f1)\n",
    "print(\"f1_score:macro:\",train_B_f2)\n",
    "print(\"f1_score:micro:\",train_B_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_B, clf_B.predict_proba(X_train_transformed_B), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy for Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.325\n",
      "f1_score:Weighted: 0.3096590909090909\n",
      "f1_score:macro: 0.3096590909090909\n",
      "f1_score:micro: 0.325\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "clf_test_pred_B=clf_B.predict(X_test_transformed_B)\n",
    "clf_acc_score_test_B = accuracy_score(y_B_test, clf_test_pred_B)\n",
    "print(\"Accuracy_score:\",clf_acc_score_test_B)\n",
    "\n",
    "# F1-Score:\n",
    "test_B_f1 = f1_score(y_B_test, clf_test_pred_B, average='weighted')\n",
    "test_B_f2 = f1_score(y_B_test, clf_test_pred_B, average='macro')\n",
    "test_B_f3 = f1_score(y_B_test, clf_test_pred_B, average='micro')\n",
    "print(\"f1_score:Weighted:\",test_B_f1)\n",
    "print(\"f1_score:macro:\",test_B_f2)\n",
    "print(\"f1_score:micro:\",test_B_f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6475000000000001"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_B_test, clf_B.predict_proba(X_test_transformed_B), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
