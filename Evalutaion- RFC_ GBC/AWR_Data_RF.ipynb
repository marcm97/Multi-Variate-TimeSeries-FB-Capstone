{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done using Parameters tuning For AWR  since in the inital stage got contarcting reulst fro both geraient boosting and GBC:\n",
    "## Hence to evluate results more precisely doign parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_series/_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_hierarchical/_check.py:50: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_hierarchical/_check.py:51: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_panel/_check.py:48: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/Users/dinsind/.local/lib/python3.8/site-packages/sktime/datatypes/_panel/_check.py:49: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n"
     ]
    }
   ],
   "source": [
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sktime.datatypes._panel._convert import from_nested_to_3d_numpy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from pyts.datasets import load_basic_motions\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"train\", return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test= load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"test\", return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 9, 144)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3d = from_nested_to_3d_numpy(X)\n",
    "X_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 9, 144)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_3d= from_nested_to_3d_numpy(X_test)\n",
    "X_test_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reservoir_features:\n",
    "    '''\n",
    "    creates an object associated with a multivariate dataset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self,data,num_features):\n",
    "        '''\n",
    "        data: Takes in a multidimensional array (x * y * z) - z>y\n",
    "        Initializes it\n",
    "        x: Timeseries\n",
    "        y: Attributes for a given timeseries observation\n",
    "        z: timestamped observations (features)\n",
    "        \n",
    "        num_features: you must specify the dimension you want to reduce it to\n",
    "        \n",
    "        '''\n",
    "        self.features = []\n",
    "        self.filters_used = []\n",
    "        self.original_data = data.copy()\n",
    "        self.data = data.copy()\n",
    "        self.num_features = num_features\n",
    "        self.x = data.shape[0]\n",
    "        self.y = data.shape[1]\n",
    "        self.z = data.shape[2]\n",
    "        # perform checks \n",
    "        #1. 3d numpy array\n",
    "        #2. Each time series should have same number of observations\n",
    "        #3. num_features should be less than timestamped observations\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Each attribute could potentially be on a different scale\n",
    "        modifies the original data and performs a min max normalization\n",
    "        '''\n",
    "        for i in range(self.original_data.shape[0]):\n",
    "            for j in range(self.original_data.shape[1]):\n",
    "                self.data[i][j] = (self.original_data[i][j] - self.original_data[i][j].min())/(self.original_data[i][j].max()-self.original_data[i][j].min())\n",
    "    \n",
    "    \n",
    "    def filters(self,stride_len = [1], num_filters = 1):\n",
    "        '''\n",
    "        stride_len: num of columns to skip after each filter multiplication\n",
    "        num_filters: you can specify the number of filters you need; each filter will be of a differnt size\n",
    "        size of filter = n*m \n",
    "        (n = # of rows = attribute size, \n",
    "        m = # of columns)\n",
    "        '''\n",
    "        #Have error check to make sure stride len is a list and value is <length of attributes\n",
    "        n = self.y\n",
    "        \n",
    "        \n",
    "        #Edge case vals is empty/smaller than num_filters\n",
    "        \n",
    "        for iteration in range(num_filters):\n",
    "            m = self._get_m(stride_len[iteration])\n",
    "            filter_a = np.random.random((n,m))\n",
    "            print(\"filter of size \", str(n), \"*\", str(m), \"was created\\n\")\n",
    "            self.filters_used.append(filter_a)\n",
    "            \n",
    "            temp_features =[]\n",
    "            for i in range(self.x):\n",
    "                temp = []\n",
    "                j = 0\n",
    "                while j + m < self.data.shape[2]:\n",
    "                    temp.append((filter_a*self.data[i,:,j:j+m]).mean())\n",
    "                    j+=stride_len[iteration]\n",
    "                temp_features.append(temp)\n",
    "            self.features.append(temp_features)\n",
    "    \n",
    "    \n",
    "    def _get_m(self,stride_len):\n",
    "        '''\n",
    "        stride_len: \n",
    "        based on stride length,& num_features, we calculate possible filter size \n",
    "        '''\n",
    "        m = self.z -(self.num_features)*stride_len\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    def result_features(self):\n",
    "        '''\n",
    "        if multiple filters were added, takes the average result\n",
    "        '''\n",
    "        ans =[]\n",
    "        for timeseries in range(len(self.features[0])):\n",
    "            temp =[]\n",
    "            for feature in range(len(self.features[0][0])):\n",
    "                val = np.mean([self.features[filter][timeseries][feature] for filter in range(len(self.features))])\n",
    "                temp.append(val)\n",
    "            ans.append(temp)\n",
    "        return ans\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, num_features, stride_len, num_filters):\n",
    "    data_transformed = reservoir_features(data ,num_features = num_features)\n",
    "    #normalize\n",
    "    data_transformed.normalize()\n",
    "    #create 2 filters\n",
    "    data_transformed.filters(stride_len = stride_len, num_filters = num_filters)\n",
    "    data_transformed = data_transformed.result_features()\n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  9 * 24 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_transformed= transform_data(X_3d, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  9 * 24 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed=transform_data(X_test_3d, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "Best accuracy: 0.571\n",
      "\n",
      "Best params:\n",
      " {'bootstrap': True, 'max_depth': 110, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Create the grid:\n",
    "gs_rf = GridSearchCV(forest, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit using grid search:\n",
    "gs_rf.fit(X_train_transformed, y)\n",
    "\n",
    "# Print best accuracy and best parameters:\n",
    "print('Best accuracy: %.3f' % gs_rf.best_score_)\n",
    "print('\\nBest params:\\n', gs_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5633333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_test_pred=gs_rf.predict(X_test_transformed)\n",
    "clf_test_acc_score = accuracy_score(y_test, clf_test_pred)\n",
    "print(clf_test_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536226851851852"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, gs_rf.predict_proba(X_test_transformed), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=100, max_features=2, min_samples_leaf=4,\n",
       "                       min_samples_split=8, n_estimators=200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_C= RandomForestClassifier(bootstrap= True, max_depth=100, max_features=2,min_samples_leaf=4, min_samples_split=8,\n",
    "                              n_estimators=200)\n",
    "\n",
    "clf_C.fit(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9233333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_test_pred=clf_C.predict(X_test_transformed)\n",
    "clf_test_acc_score = accuracy_score(y_test, clf_test_pred)\n",
    "print(clf_test_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985648148148148"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf_C.predict_proba(X_test_transformed), multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
