{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import load_basic_motions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch = load_basic_motions()\n",
    "X_train, X_test, y_train, y_test = load_basic_motions(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reservoir_features:\n",
    "    '''\n",
    "    creates an object associated with a multivariate dataset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def __init__(self,data,num_features):\n",
    "        '''\n",
    "        data: Takes in a multidimensional array (x * y * z) - z>y\n",
    "        Initializes it\n",
    "        x: Timeseries\n",
    "        y: Attributes for a given timeseries observation\n",
    "        z: timestamped observations (features)\n",
    "        \n",
    "        num_features: you must specify the dimension you want to reduce it to\n",
    "        \n",
    "        '''\n",
    "        self.features = []\n",
    "        self.filters_used = []\n",
    "        self.original_data = data.copy()\n",
    "        self.data = data.copy()\n",
    "        self.num_features = num_features\n",
    "        self.x = data.shape[0]\n",
    "        self.y = data.shape[1]\n",
    "        self.z = data.shape[2]\n",
    "        # perform checks \n",
    "        #1. 3d numpy array\n",
    "        #2. Each time series should have same number of observations\n",
    "        #3. num_features should be less than timestamped observations\n",
    "    \n",
    "    def normalize(self):\n",
    "        '''\n",
    "        Each attribute could potentially be on a different scale\n",
    "        modifies the original data and performs a min max normalization\n",
    "        '''\n",
    "        for i in range(self.original_data.shape[0]):\n",
    "            for j in range(self.original_data.shape[1]):\n",
    "                self.data[i][j] = (self.original_data[i][j] - self.original_data[i][j].min())/(self.original_data[i][j].max()-self.original_data[i][j].min())\n",
    "    \n",
    "    \n",
    "    def filters(self,stride_len = [1], num_filters = 1):\n",
    "        '''\n",
    "        stride_len: num of columns to skip after each filter multiplication\n",
    "        num_filters: you can specify the number of filters you need; each filter will be of a differnt size\n",
    "        size of filter = n*m \n",
    "        (n = # of rows = attribute size, \n",
    "        m = # of columns)\n",
    "        '''\n",
    "        #Have error check to make sure stride len is a list and value is <length of attributes\n",
    "        n = self.y\n",
    "        \n",
    "        \n",
    "        #Edge case vals is empty/smaller than num_filters\n",
    "        \n",
    "        for iteration in range(num_filters):\n",
    "            m = self._get_m(stride_len[iteration])\n",
    "            filter_a = np.random.random((n,m))\n",
    "            print(\"filter of size \", str(n), \"*\", str(m), \"was created\\n\")\n",
    "            self.filters_used.append(filter_a)\n",
    "            \n",
    "            temp_features =[]\n",
    "            for i in range(self.x):\n",
    "                temp = []\n",
    "                j = 0\n",
    "                while j + m < self.data.shape[2]:\n",
    "                    temp.append((filter_a*self.data[i,:,j:j+m]).mean())\n",
    "                    j+=stride_len[iteration]\n",
    "                temp_features.append(temp)\n",
    "            self.features.append(temp_features)\n",
    "    \n",
    "    \n",
    "    def _get_m(self,stride_len):\n",
    "        '''\n",
    "        stride_len: \n",
    "        based on stride length,& num_features, we calculate possible filter size \n",
    "        '''\n",
    "        m = self.z -(self.num_features)*stride_len\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    def result_features(self):\n",
    "        '''\n",
    "        if multiple filters were added, takes the average result\n",
    "        '''\n",
    "        ans =[]\n",
    "        for timeseries in range(len(self.features[0])):\n",
    "            temp =[]\n",
    "            for feature in range(len(self.features[0][0])):\n",
    "                val = np.mean([self.features[filter][timeseries][feature] for filter in range(len(self.features))])\n",
    "                temp.append(val)\n",
    "            ans.append(temp)\n",
    "        return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "a = reservoir_features(X_train,num_features = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "a.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 82 was created\n",
      "\n",
      "filter of size  6 * 64 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create 2 filters\n",
    "a.filters(stride_len = [1,2], num_filters = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 46 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.filters(stride_len = [3], num_filters = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = a.result_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on 7 datasets using logistic regression and SVM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datsets\n",
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from sktime.datatypes._panel._convert import from_nested_to_3d_numpy\n",
    "from pyts.datasets import load_basic_motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logistic Regression on the Transformed Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 52 was created\n",
      "\n",
      "40\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#create object\n",
    "X_train_transformed = reservoir_features(X_train,num_features = 16)\n",
    "#normalize\n",
    "X_train_transformed.normalize()\n",
    "#create 2 filters\n",
    "X_train_transformed.filters(stride_len = [3], num_filters = 1)\n",
    "X_train_reservoir_transformed = X_train_transformed.result_features()\n",
    "print(len(X_train_reservoir_transformed))\n",
    "print(len(X_train_reservoir_transformed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 52 was created\n",
      "\n",
      "40\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#create object\n",
    "X_test_transformed = reservoir_features(X_test,num_features = 16)\n",
    "#normalize\n",
    "X_test_transformed.normalize()\n",
    "#create 2 filters\n",
    "X_test_transformed.filters(stride_len = [3], num_filters = 1)\n",
    "X_test_reservoir_transformed = X_test_transformed.result_features()\n",
    "print(len(X_test_reservoir_transformed))\n",
    "print(len(X_test_reservoir_transformed[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Walking', 'Badminton', 'Standing', 'Running'}\n",
      "{'Walking', 'Badminton', 'Standing', 'Running'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train))\n",
    "print(set(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a l2-regularized logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_model = LogisticRegression(random_state=0, max_iter=100, multi_class='multinomial', solver='lbfgs', C=1.0).fit(X_train_reservoir_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_model.score(X_train_reservoir_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = l2_model.predict(X_train_reservoir_transformed)\n",
    "l2_train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(l2_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = l2_model.predict(X_test_reservoir_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n"
     ]
    }
   ],
   "source": [
    "l2_test_acc = accuracy_score(y_test, y_pred)\n",
    "print(l2_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function for data transformation and baseline l2 logistic regression validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, num_features, stride_len, num_filters):\n",
    "    data_transformed = reservoir_features(data ,num_features = num_features)\n",
    "    #normalize\n",
    "    data_transformed.normalize()\n",
    "    #create 2 filters\n",
    "    data_transformed.filters(stride_len = stride_len, num_filters = num_filters)\n",
    "    data_transformed = data_transformed.result_features()\n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_l2_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    l2_model = LogisticRegression(random_state=0, max_iter=100, multi_class='multinomial', solver='lbfgs', C=1.0).fit(X_train, y_train)\n",
    "    # training metrics\n",
    "    print('train_acc: %f '%l2_model.score(X_train, y_train))\n",
    "    train_auc = roc_auc_score(y_train, l2_model.predict_proba(X_train), multi_class='ovr')\n",
    "    print('train_AUC: %f '%train_auc)\n",
    "    # testing metrics\n",
    "    y_pred = l2_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print('test_acc: %f '%test_acc)\n",
    "    test_auc = roc_auc_score(y_test, l2_model.predict_proba(X_test), multi_class='ovr')\n",
    "    print('test_AUC: %f '%test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_BM, X_test_BM, y_train_BM, y_test_BM = load_basic_motions(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 52 was created\n",
      "\n",
      "filter of size  6 * 52 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_BM = transform_data(X_train_BM, 16, [3], 1)\n",
    "X_test_BM = transform_data(X_test_BM, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.450000 \n",
      "train_AUC: 0.712500 \n",
      "test_acc: 0.350000 \n",
      "test_AUC: 0.672500 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_BM, y_train_BM, X_test_BM, y_test_BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SVM on the Transformed Output\n",
    "For multi-class classification, try both the one_vs_one(ovo) method and the one_vs_rest(ovr) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the baseline SVM model using the one_vs_one method on the basic_motions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(decision_function_shape='ovo')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_vs_one_svc_clf = svm.SVC(C=1.0, decision_function_shape='ovo')\n",
    "one_vs_one_svc_clf.fit(X_train_BM, y_train_BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_BM = one_vs_one_svc_clf.predict(X_train_BM)\n",
    "ovo_svc_train_clf_acc = accuracy_score(y_train_BM, y_train_pred_BM)\n",
    "print(ovo_svc_train_clf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n"
     ]
    }
   ],
   "source": [
    "y_pred_BM = one_vs_one_svc_clf.predict(X_test_BM)\n",
    "ovo_svc_test_clf_acc = accuracy_score(y_test_BM, y_pred_BM)\n",
    "print(ovo_svc_test_clf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the baseline SVM model using the one_vs_rest method on the basic_motions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_vs_rest_svc_clf = svm.SVC(decision_function_shape='ovr')\n",
    "one_vs_rest_svc_clf.fit(X_train_BM, y_train_BM)\n",
    "# 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_BM = one_vs_rest_svc_clf.predict(X_train_BM)\n",
    "ovr_svc_train_clf_acc = accuracy_score(y_train_BM, y_train_pred_BM)\n",
    "print(ovr_svc_train_clf_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35\n"
     ]
    }
   ],
   "source": [
    "y_pred_BM = one_vs_rest_svc_clf.predict(X_test_BM)\n",
    "ovr_svc_test_clf_acc = accuracy_score(y_test_BM, y_pred_BM)\n",
    "print(ovr_svc_test_clf_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a framework for parameter tuning on SVM models <br>\n",
    "SVM parameters: C, kernel, degree (polynomial only), decision_function, break_ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svc_acc(X_train, y_train, X_test, y_test, C_select, kernel_select, d_select, df_select):\n",
    "    model = svm.SVC(C=C_select, kernel=kernel_select, degree=d_select, decision_function_shape=df_select, probability=True).fit(X_train, y_train)\n",
    "    # training metrics\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    if len(set(y_train)) == 2:\n",
    "        train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:,1], multi_class='ovo')\n",
    "    else:\n",
    "        train_auc = roc_auc_score(y_train, model.predict_proba(X_train), multi_class='ovr')\n",
    "    train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "    # testing metrics\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    if len(set(y_train)) == 2:\n",
    "        test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], multi_class='ovo')\n",
    "    else:\n",
    "        test_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
    "    test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "    return 'train_acc: %f '%train_acc + 'train_auc: %f '%train_auc + 'train_f1: %f '%train_f1 + 'test_acc: %f '%test_acc + 'test_auc: %f '%test_auc + 'test_f1: %f '%test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_SVM_CV_output(f_name, C_list, kernel_list, d_list, df_list, X_train, y_train, X_test, y_test):\n",
    "    with open(f_name, 'w') as f:\n",
    "        for C in C_list:\n",
    "            for kernel in kernel_list:\n",
    "                for df in df_list:\n",
    "                    if kernel == 'poly':\n",
    "                        for d in d_list:\n",
    "                            f.write('C = %f'%C)\n",
    "                            f.write('\\n')\n",
    "                            f.write('kernel = polynomial')\n",
    "                            f.write('\\n')\n",
    "                            f.write('decision_function = '+df)\n",
    "                            f.write('\\n')\n",
    "                            f.write('degree = %d'%d)\n",
    "                            f.write('\\n')\n",
    "                            f.write(get_svc_acc(X_train, y_train, X_test, y_test, C, kernel, d, df))\n",
    "                            f.write('\\n')\n",
    "                            f.write('\\n')\n",
    "                    else:\n",
    "                        f.write('C = %f'%C)\n",
    "                        f.write('\\n')\n",
    "                        f.write('kernel = '+kernel)\n",
    "                        f.write('\\n')\n",
    "                        f.write('decision_function = '+df)\n",
    "                        f.write('\\n')\n",
    "                        f.write(get_svc_acc(X_train, y_train, X_test, y_test, C, kernel, 0, df))\n",
    "                        f.write('\\n')\n",
    "                        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output CV result for basic_motions dataset\n",
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "d_list = [2, 3, 4]\n",
    "df_list = ['ovr', 'ovo']\n",
    "write_SVM_CV_output('basic_motions_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_BM, y_train_BM, X_test_BM, y_test_BM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train, y_train, X_test, y_test):\n",
    "    best_test_acc = None\n",
    "    best_test_auc = None\n",
    "    best_test_f1 = None\n",
    "    best_acc_result_list = []\n",
    "    best_auc_result_list = []\n",
    "    best_f1_result_list = []\n",
    "    for C in C_list:\n",
    "        for kernel in kernel_list:\n",
    "            for df in df_list:\n",
    "                if kernel == 'poly':\n",
    "                    for d in d_list:\n",
    "                        model = svm.SVC(C=C, kernel=kernel, degree=d, decision_function_shape=df, probability=True).fit(X_train, y_train)\n",
    "                        # training metrics\n",
    "                        train_pred = model.predict(X_train)\n",
    "                        train_acc = accuracy_score(y_train, train_pred)\n",
    "                        if len(set(y_train)) == 2:\n",
    "                            train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:,1], multi_class='ovo')\n",
    "                        else:\n",
    "                            train_auc = roc_auc_score(y_train, model.predict_proba(X_train), multi_class='ovr')\n",
    "                        train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "                        # testing metrics\n",
    "                        test_pred = model.predict(X_test)\n",
    "                        test_acc = accuracy_score(y_test, test_pred)\n",
    "                        if len(set(y_train)) == 2:\n",
    "                            test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], multi_class='ovo')\n",
    "                        else:\n",
    "                            test_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
    "                        test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "                        if not best_test_acc or best_test_acc <= test_acc:\n",
    "                            best_test_acc = test_acc\n",
    "                            best_acc_result_list = [C, kernel, d, df, train_acc, test_acc]\n",
    "                        if not best_test_auc or best_test_auc <= test_auc:\n",
    "                            best_test_auc = test_auc\n",
    "                            best_auc_result_list = [C, kernel, d, df, train_auc, test_auc] \n",
    "                        if not best_test_f1 or best_test_f1 <= test_f1:\n",
    "                            best_test_f1 = test_f1\n",
    "                            best_f1_result_list = [C, kernel, d, df, train_f1, test_f1]\n",
    "                else:\n",
    "                    model = svm.SVC(C=C, kernel=kernel, decision_function_shape=df, probability=True).fit(X_train, y_train)\n",
    "                    # training metrics\n",
    "                    train_pred = model.predict(X_train)\n",
    "                    train_acc = accuracy_score(y_train, train_pred)\n",
    "                    if len(set(y_train)) == 2:\n",
    "                        train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:,1], multi_class='ovo')\n",
    "                    else:\n",
    "                        train_auc = roc_auc_score(y_train, model.predict_proba(X_train), multi_class='ovr')\n",
    "                    train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "                    # testing metrics\n",
    "                    test_pred = model.predict(X_test)\n",
    "                    if len(set(y_train)) == 2:\n",
    "                        test_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1], multi_class='ovo')\n",
    "                    else:\n",
    "                        test_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
    "                    test_acc = accuracy_score(y_test, test_pred)\n",
    "                    test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "                    if not best_test_acc or best_test_acc <= test_acc:\n",
    "                        best_test_acc = test_acc\n",
    "                        best_acc_result_list = [C, kernel, 0, df, train_acc, test_acc]\n",
    "                    if not best_test_auc or best_test_auc <= test_auc:\n",
    "                        best_test_auc = test_auc\n",
    "                        best_auc_result_list = [C, kernel, 0, df, train_auc, test_auc]\n",
    "                    if not best_test_f1 or best_test_f1 <= test_f1:\n",
    "                        best_test_f1 = test_f1\n",
    "                        best_f1_result_list = [C, kernel, 0, df, train_f1, test_f1]\n",
    "    return (best_acc_result_list, best_auc_result_list, best_f1_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 'poly', 2, 'ovo', 0.95, 0.5],\n",
       " [1.0, 'poly', 2, 'ovr', 0.9916666666666667, 0.72],\n",
       " [1.0, 'poly', 2, 'ovo', 0.949874686716792, 0.4825274725274725])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_BM, y_train_BM, X_test_BM, y_test_BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation using UCR_UEA Datasets\n",
    "There are six total UCR_UEA datasets - ArticularyWordRecognition, AtrialFibrillation, Cricket, Epilepsy, FingerMovements and Handwriting. <br>\n",
    "For each dataset, we run validation using the baseline L2 logistic regression model and SVM models with parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the ArticularyWordRecognition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 9)\n",
      "(275,)\n"
     ]
    }
   ],
   "source": [
    "X_train_WR, y_train_WR = load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"train\", return_X_y=True)\n",
    "print(X_train_WR.shape)\n",
    "print(y_train_WR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 9)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "X_test_WR, y_test_WR = load_UCR_UEA_dataset('ArticularyWordRecognition', split=\"test\", return_X_y=True)\n",
    "print(X_test_WR.shape)\n",
    "print(y_test_WR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5.0', '24.0', '14.0', '6.0', '11.0', '15.0', '4.0', '20.0', '13.0', '19.0', '7.0', '1.0', '16.0', '10.0', '21.0', '9.0', '25.0', '8.0', '23.0', '17.0', '3.0', '12.0', '2.0', '18.0', '22.0'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train_WR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 9, 144)\n",
      "(300, 9, 144)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_WR = from_nested_to_3d_numpy(X_train_WR)\n",
    "X_test_3d_WR = from_nested_to_3d_numpy(X_test_WR)\n",
    "\n",
    "print(X_train_3d_WR.shape)\n",
    "print(X_test_3d_WR.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  9 * 24 was created\n",
      "\n",
      "filter of size  9 * 24 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_WR = transform_data(X_train_3d_WR, 40, [3], 1)\n",
    "X_test_WR = transform_data(X_test_3d_WR, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.458182 \n",
      "train_AUC: 0.927851 \n",
      "test_acc: 0.460000 \n",
      "test_AUC: 0.913762 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_WR, y_train_WR, X_test_WR, y_test_WR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "d_list = [2,3,4]\n",
    "df_list = ['ovr', 'ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('word_recognition_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_WR, y_train_WR, X_test_WR, y_test_WR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 'rbf', 0, 'ovo', 0.7163636363636363, 0.5166666666666667],\n",
       " [0.1, 'poly', 3, 'ovr', 0.9956887052341599, 0.946701388888889],\n",
       " [1.0, 'rbf', 0, 'ovo', 0.7055725943722991, 0.4807748030824627])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_WR, y_train_WR, X_test_WR, y_test_WR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the AtrialFibrillation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "X_train_AF, y_train_AF = load_UCR_UEA_dataset('AtrialFibrillation', split=\"train\", return_X_y=True)\n",
    "print(X_train_AF.shape)\n",
    "print(y_train_AF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "X_test_AF, y_test_AF = load_UCR_UEA_dataset('AtrialFibrillation', split=\"test\", return_X_y=True)\n",
    "print(X_test_AF.shape)\n",
    "print(y_test_AF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n' 'n' 'n' 'n' 'n' 's' 's' 's' 's' 's' 't' 't' 't' 't' 't']\n"
     ]
    }
   ],
   "source": [
    "print(y_train_AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2, 640)\n",
      "(15, 2, 640)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_AF = from_nested_to_3d_numpy(X_train_AF)\n",
    "X_test_3d_AF = from_nested_to_3d_numpy(X_test_AF)\n",
    "\n",
    "print(X_train_3d_AF.shape)\n",
    "print(X_test_3d_AF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  2 * 520 was created\n",
      "\n",
      "filter of size  2 * 520 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_AF = transform_data(X_train_3d_AF, 40, [3], 1)\n",
    "X_test_AF = transform_data(X_test_3d_AF, 40, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_AF))\n",
    "print(len(X_train_AF[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.400000 \n",
      "train_AUC: 0.660000 \n",
      "test_acc: 0.266667 \n",
      "test_AUC: 0.486667 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_AF, y_train_AF, X_test_AF, y_test_AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "d_list = [2, 3, 4]\n",
    "df_list = ['ovr', 'ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('atrial_fibrillation_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_AF, y_train_AF, X_test_AF, y_test_AF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.001, 'sigmoid', 0, 'ovo', 0.3333333333333333, 0.3333333333333333],\n",
       " [0.01, 'rbf', 0, 'ovo', 0.25333333333333335, 0.6466666666666667],\n",
       " [1.0, 'poly', 4, 'ovo', 1.0, 0.3333333333333333])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_AF, y_train_AF, X_test_AF, y_test_AF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the Cricket dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 6)\n",
      "(108,)\n"
     ]
    }
   ],
   "source": [
    "X_train_C, y_train_C = load_UCR_UEA_dataset('Cricket', split=\"train\", return_X_y=True)\n",
    "print(X_train_C.shape)\n",
    "print(y_train_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 6)\n",
      "(72,)\n"
     ]
    }
   ],
   "source": [
    "X_test_C, y_test_C = load_UCR_UEA_dataset('Cricket', split=\"test\", return_X_y=True)\n",
    "print(X_test_C.shape)\n",
    "print(y_test_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'10.0', '12.0', '4.0', '5.0', '9.0', '2.0', '6.0', '11.0', '7.0', '1.0', '8.0', '3.0'}\n",
      "{'10.0', '12.0', '4.0', '5.0', '9.0', '2.0', '6.0', '11.0', '7.0', '1.0', '8.0', '3.0'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train_C))\n",
    "print(set(y_test_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 6, 1197)\n",
      "(72, 6, 1197)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_C = from_nested_to_3d_numpy(X_train_C)\n",
    "X_test_3d_C = from_nested_to_3d_numpy(X_test_C)\n",
    "\n",
    "print(X_train_3d_C.shape)\n",
    "print(X_test_3d_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  6 * 597 was created\n",
      "\n",
      "filter of size  6 * 597 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_C = transform_data(X_train_3d_C, 200, [3], 1)\n",
    "X_test_C = transform_data(X_test_3d_C, 200, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.388889 \n",
      "train_AUC: 0.895062 \n",
      "test_acc: 0.263889 \n",
      "test_AUC: 0.861111 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_C, y_train_C, X_test_C, y_test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "d_list = [2, 3, 4]\n",
    "df_list = ['ovr', 'ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('cricket_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_C, y_train_C, X_test_C, y_test_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 'poly', 2, 'ovo', 0.9074074074074074, 0.6388888888888888],\n",
       " [1.0, 'poly', 2, 'ovo', 0.9891507669285448, 0.943392255892256],\n",
       " [1.0, 'poly', 2, 'ovo', 0.90614608416466, 0.6153721278721278])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_C, y_train_C, X_test_C, y_test_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the Epilepsy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3)\n",
      "(137,)\n"
     ]
    }
   ],
   "source": [
    "X_train_E, y_train_E = load_UCR_UEA_dataset('Epilepsy', split=\"train\", return_X_y=True)\n",
    "print(X_train_E.shape)\n",
    "print(y_train_E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 3)\n",
      "(138,)\n"
     ]
    }
   ],
   "source": [
    "X_test_E, y_test_E = load_UCR_UEA_dataset('Epilepsy', split=\"test\", return_X_y=True)\n",
    "print(X_test_E.shape)\n",
    "print(y_test_E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epilepsy', 'running', 'walking', 'sawing'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3, 206)\n",
      "(138, 3, 206)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_E = from_nested_to_3d_numpy(X_train_E)\n",
    "X_test_3d_E = from_nested_to_3d_numpy(X_test_E)\n",
    "\n",
    "print(X_train_3d_E.shape)\n",
    "print(X_test_3d_E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  3 * 158 was created\n",
      "\n",
      "filter of size  3 * 158 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_E = transform_data(X_train_3d_E, 16, [3], 1)\n",
    "X_test_E = transform_data(X_test_3d_E, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.299270 \n",
      "train_AUC: 0.572439 \n",
      "test_acc: 0.268116 \n",
      "test_AUC: 0.509503 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_E, y_train_E, X_test_E, y_test_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SVM model using polynomial kernel for this dataset runs forever for degree=4, remove it\n",
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "d_list = [2, 3]\n",
    "df_list = ['ovr', 'ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('epilepsy_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_E, y_train_E, X_test_E, y_test_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 'poly', 3, 'ovo', 0.7372262773722628, 0.35507246376811596],\n",
       " [1.0, 'poly', 3, 'ovr', 0.7582429115642032, 0.591304260062147],\n",
       " [1.0, 'poly', 3, 'ovo', 0.7352078493069657, 0.3567753145423581])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_E, y_train_E, X_test_E, y_test_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the FingerMovements dataset\n",
    "Note that this is a binary classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 28)\n",
      "(316,)\n"
     ]
    }
   ],
   "source": [
    "X_train_FM, y_train_FM = load_UCR_UEA_dataset('FingerMovements', split=\"train\", return_X_y=True)\n",
    "print(X_train_FM.shape)\n",
    "print(y_train_FM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X_test_FM, y_test_FM = load_UCR_UEA_dataset('FingerMovements', split=\"test\", return_X_y=True)\n",
    "print(X_test_FM.shape)\n",
    "print(y_test_FM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 28)\n",
      "(416,)\n",
      "(332, 28)\n",
      "(332,)\n",
      "(84, 28)\n",
      "(84,)\n"
     ]
    }
   ],
   "source": [
    "X_FM, y_FM = load_UCR_UEA_dataset('FingerMovements', return_X_y=True)\n",
    "print(X_FM.shape)\n",
    "print(y_FM.shape)\n",
    "X_train_FM, X_test_FM, y_train_FM, y_test_FM = train_test_split(X_FM, y_FM, test_size=0.2, random_state=42)\n",
    "print(X_train_FM.shape)\n",
    "print(y_train_FM.shape)\n",
    "print(X_test_FM.shape)\n",
    "print(y_test_FM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'right', 'left'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train_FM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(332, 28, 50)\n",
      "(84, 28, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_FM = from_nested_to_3d_numpy(X_train_FM)\n",
    "X_test_3d_FM = from_nested_to_3d_numpy(X_test_FM)\n",
    "\n",
    "print(X_train_3d_FM.shape)\n",
    "print(X_test_3d_FM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  28 * 2 was created\n",
      "\n",
      "filter of size  28 * 2 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_FM = transform_data(X_train_3d_FM, 16, [3], 1)\n",
    "X_test_FM = transform_data(X_test_3d_FM, 16, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.512048 \n",
      "train_AUC: 0.556936 \n",
      "test_acc: 0.440476 \n",
      "test_AUC: 0.480161 \n"
     ]
    }
   ],
   "source": [
    "# The run_baseline_l2 function should not be run here since it is binary classification\n",
    "l2_model_FM = LogisticRegression(random_state=0, max_iter=100, C=1.0).fit(X_train_FM, y_train_FM)\n",
    "# training metrics\n",
    "print('train_acc: %f '%l2_model_FM.score(X_train_FM, y_train_FM))\n",
    "train_auc = roc_auc_score(y_train_FM, l2_model_FM.predict_proba(X_train_FM)[:, 1], multi_class='ovr')\n",
    "print('train_AUC: %f '%train_auc)\n",
    "# testing metrics\n",
    "y_pred_FM = l2_model_FM.predict(X_test_FM)\n",
    "test_acc = accuracy_score(y_test_FM, y_pred_FM)\n",
    "print('test_acc: %f '%test_acc)\n",
    "test_auc = roc_auc_score(y_test_FM, l2_model.predict_proba(X_test_FM)[:, 1], multi_class='ovr')\n",
    "print('test_AUC: %f '%test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the ovo method is needed here\n",
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "d_list = [2, 3]\n",
    "df_list = ['ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('finger_movements_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_FM, y_train_FM, X_test_FM, y_test_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.01, 'poly', 2, 'ovo', 0.5903614457831325, 0.5714285714285714],\n",
       " [0.1, 'poly', 2, 'ovo', 0.6278377102175728, 0.5451408855664175],\n",
       " [0.01, 'poly', 2, 'ovo', 0.5863546851498658, 0.5626021575678326])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_FM, y_train_FM, X_test_FM, y_test_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation using the Handwriting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "X_train_H, y_train_H = load_UCR_UEA_dataset('Handwriting', split=\"train\", return_X_y=True)\n",
    "print(X_train_H.shape)\n",
    "print(y_train_H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 3)\n",
      "(850,)\n"
     ]
    }
   ],
   "source": [
    "X_test_H, y_test_H = load_UCR_UEA_dataset('Handwriting', split=\"test\", return_X_y=True)\n",
    "print(X_test_H.shape)\n",
    "print(y_test_H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "(1000,)\n",
      "(500, 3)\n",
      "(500,)\n",
      "(500, 3)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "X_H, y_H = load_UCR_UEA_dataset('Handwriting', return_X_y=True)\n",
    "print(X_H.shape)\n",
    "print(y_H.shape)\n",
    "X_train_H, X_test_H, y_train_H, y_test_H = train_test_split(X_H, y_H, test_size=0.5, random_state=42)\n",
    "print(X_train_H.shape)\n",
    "print(y_train_H.shape)\n",
    "print(X_test_H.shape)\n",
    "print(y_test_H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'18.0', '7.0', '10.0', '9.0', '15.0', '21.0', '8.0', '12.0', '20.0', '4.0', '1.0', '25.0', '17.0', '3.0', '26.0', '23.0', '14.0', '6.0', '19.0', '24.0', '11.0', '2.0', '5.0', '16.0', '22.0', '13.0'}\n",
      "{'18.0', '7.0', '10.0', '9.0', '15.0', '21.0', '8.0', '12.0', '4.0', '20.0', '1.0', '17.0', '25.0', '3.0', '26.0', '23.0', '14.0', '6.0', '19.0', '24.0', '11.0', '2.0', '5.0', '16.0', '22.0', '13.0'}\n"
     ]
    }
   ],
   "source": [
    "print(set(y_train_H))\n",
    "print(set(y_test_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3, 152)\n",
      "(500, 3, 152)\n"
     ]
    }
   ],
   "source": [
    "X_train_3d_H = from_nested_to_3d_numpy(X_train_H)\n",
    "X_test_3d_H = from_nested_to_3d_numpy(X_test_H)\n",
    "\n",
    "print(X_train_3d_H.shape)\n",
    "print(X_test_3d_H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter of size  3 * 122 was created\n",
      "\n",
      "filter of size  3 * 122 was created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_H = transform_data(X_train_3d_H, 10, [3], 1)\n",
    "X_test_H = transform_data(X_test_3d_H, 10, [3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.072000 \n",
      "train_AUC: 0.589020 \n",
      "test_acc: 0.034000 \n",
      "test_AUC: 0.553107 \n"
     ]
    }
   ],
   "source": [
    "run_baseline_l2_logistic_regression(X_train_H, y_train_H, X_test_H, y_test_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_list = [1.0, 0.1, 0.01, 0.001]\n",
    "kernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "d_list = [2, 3]\n",
    "df_list = ['ovr', 'ovo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_SVM_CV_output('handwriting_svm_cv_results.txt', C_list, kernel_list, d_list, df_list, X_train_H, y_train_H, X_test_H, y_test_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.01, 'poly', 3, 'ovo', 0.24, 0.082],\n",
       " [1.0, 'poly', 2, 'ovr', 0.7412776724218353, 0.6056194482463202],\n",
       " [1.0, 'poly', 3, 'ovo', 0.30548493431510154, 0.07014694657357551])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_SVM_models(C_list, kernel_list, d_list, df_list, X_train_H, y_train_H, X_test_H, y_test_H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
